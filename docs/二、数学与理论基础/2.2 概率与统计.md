# 概率与统计
除了线性代数，概率与统计同样在机器学习中举足轻重，可以说他俩是哥们。

在机器学习中，数据往往存在不确定性和噪声。

概率与统计提供了描述、分析和推断这种不确定性的数学工具，是理解模型原理、设计算法和评估性能的基础。无论是分类、回归，还是聚类和生成模型，概率与统计的思想都无处不在。

接下来，你应该还没学够吧！继续！

## 概率基础
### 概率
概率是对事件发生可能性的量化。设样本空间为 $$\Omega$$，事件为 $$A \subseteq \Omega$$，则事件 A的概率记作P(A)，满足：

$$0≤P(A)≤1,P(Ω)=1$$

**示例**
- 抛掷一枚公平硬币，事件**正面朝上**的概率：$$P(\text{正面}) = \frac{1}{2}$$
- 抛掷两枚硬币，事件**至少一枚正面**的概率：$$P(\text{至少一正}) = 1 - P(\text{全反}) = 1 - \frac{1}{4} = \frac{3}{4}$$

### 条件概率
条件概率用于描述在事件 $$B$$ 已经发生的前提下，事件 $$A$$ 发生的概率：$$P(A|B) = \frac{P(A \cap B)}{P(B)}, \quad P(B) > 0$$

直观点理解就是如果已知某些信息（事件 $$B$$ 已发生），条件概率告诉我们事件 $$A$$ 发生的可能性如何改变。

**示例**
- 在一个袋子中有 3 个红球和 2 个蓝球。
  - 事件 $$A$$：抽到红球
  - 事件 $$B$$：第一次抽到红球后不放回
  - 第二次抽到红球的条件概率：
$$P(\text{第二次红}| \text{第一次红}) = \frac{2}{4} = \frac{1}{2}$$

## 概率分布
在机器学习中，数据往往是随机的，存在噪声和不确定性。概率分布提供了描述数据如何生成和变化的数学工具，是模型设计、推断和评估的基础。理解概率分布，有助于选择合适的模型、设计算法和分析结果可靠性。

### 概率分布基础
概率分布描述随机变量可能取值及其对应概率。根据随机变量类型，可以分为离散分布和连续分布：
- 离散随机变量：取有限或可数的值
 例如掷骰子的点数 $$X$$ 服从均匀分布，$$X∈{1,2,3,4,5,6}$$
- 连续随机变量：取无限多个可能值
 例如身高、体重、温度等


概率分布用函数表示：
- 离散：概率质量函数 (PMF)  $$P(X=x)=p(x)$$
- 连续：概率密度函数 (PDF)   $$P(a \le X \le b) = \int_a^b f(x) dx$$

### 常见概率分布及其在机器学习中的应用
**1.离散分布**

**(1)伯努利分布**
  - 描述二分类随机事件，例如抛硬币
$$P(X=1)=p, \quad P(X=0)=1-p$$
  - 应用：逻辑回归的输出建模

**(2)多项分布**
  - 多类别事件的推广
$$P(X = k) = \frac{n!}{x_1! x_2! \dots x_k!} p_1^{x_1} \dots p_k^{x_k}$$
  - 应用：朴素贝叶斯分类器的特征建模
  
**(3)泊松分布**
  - 描述单位时间或空间内事件发生次数
$$P(X=k) = \frac{\lambda^k e^{-\lambda}}{k!}$$
  - 应用：事件计数预测、异常检测

**2.连续分布**

**(1)高斯分布（正态分布）**
  - 最常用的连续分布
$$f(x|\mu,\sigma^2) = \frac{1}{\sqrt{2\pi \sigma^2}} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)$$
  - 应用：
    - 回归模型残差建模
    - PCA 降维
    - 高斯混合模型（GMM）

**(2)均匀分布**
  - 所有取值等概率
$$f(x) = \frac{1}{b-a}, \quad a \le x \le b$$
  - 应用：初始化神经网络权重、随机采样

**(3)指数分布**        
  - 描述事件间隔时间
$$f(x|\lambda) = \lambda e^{-\lambda x}, \quad x \ge 0$$
  - 应用：生存分析、寿命建模

**(4)Beta 与 Dirichlet 分布**  
  - Beta 分布：用于建模概率（0~1之间）
  - Dirichlet 分布：多类别概率建模
  - 应用：贝叶斯先验、主题模型（LDA）

### 概率分布的作用
1. 数据建模：描述数据的生成过程，例如假设回归残差服从高斯分布
2. 生成模型：生成新样本，如 GMM、VAE
3. 参数估计：利用极大似然估计（MLE）或贝叶斯推断估计模型参数
4. 不确定性量化：通过分布预测概率，而不仅仅是点值

## 条件概率与贝叶斯定理
在机器学习中，理解变量之间的依赖关系和如何在已有信息下更新预测是核心问题。条件概率和贝叶斯定理提供了数学框架，使模型能够在不确定环境下进行合理推断和决策。

### 条件概率
条件概率用于描述在事件 $$ B $$ 已经发生的前提下，事件 $$ A $$ 发生的概率：$$P(A|B) = \frac{P(A \cap B)}{P(B)}, \quad P(B) > 0$$

直观理解就是：
- 条件概率告诉我们已知某些信息后，事件发生的可能性如何变化。
- 在机器学习中，它是理解特征与标签之间关系、序列依赖和生成模型的基础。

**示例**

- 有一个装有 3 个红球和 2 个蓝球的袋子。
  - 事件 $$A$$：抽到红球
  - 事件 $$B$$：第一次抽到红球后不放回
- 第二次抽到红球的条件概率：$$P(\text{第二次红}| \text{第一次红}) = \frac{2}{4} = \frac{1}{2}$$

### 贝叶斯定理
贝叶斯定理是条件概率的推论，用于在已有信息下更新事件发生的概率：$$P(A|B) = \frac{P(B|A) P(A)}{P(B)}$$
- $$P(A)$$：先验概率（事前对事件 $$A$$ 的信念）
- $$P(B|A)$$：似然（在事件 $$A$$ 下观测到 $$B$$ 的概率）
- $$P(A|B)$$：后验概率（在观测到 $$B$$ 后对事件 $$A$$ 的更新信念）

贝叶斯定理告诉我们：如何结合已有知识（先验）和观测数据（似然）来更新预测（后验）。

### 在机器学习中的应用
**朴素贝叶斯分类器**
- 假设特征条件独立，利用贝叶斯定理计算类别后验概率：$$P(C_k | X) \propto P(C_k) \prod_{i=1}^n P(x_i | C_k)$$
- 对每个样本计算后验概率，选择概率最大的类别作为预测结果。
- 应用：垃圾邮件分类、文本情感分析、疾病诊断。

**高斯贝叶斯分类**
- 假设每个类别条件特征服从高斯分布：$$P(x_i | C_k) = \frac{1}{\sqrt{2\pi \sigma_{k,i}^2}} \exp\left(-\frac{(x_i-\mu_{k,i})^2}{2\sigma_{k,i}^2}\right)$$
- 使用贝叶斯公式计算后验概率，实现分类。

**生成模型**
**高斯混合模型（GMM）**
- 利用后验概率 $$P(簇∣X)$$ 来分配样本到簇
- 每个簇用高斯分布表示，参数包括均值 $$\mu$$ 和协方差 $$\Sigma$$
- 应用：数据聚类、密度估计


贝叶斯定理在这些模型中提供了对未知变量的推断能力。

**贝叶斯优化**
- 超参数搜索时，将超参数视为随机变量
- 利用先验和观测数据更新分布，选择最有可能提升模型性能的参数
$$P(\text{性能最优的参数} | \text{历史评估}) \propto P(\text{历史评估} | \text{参数}) P(\text{参数})$$

### 条件独立性
贝叶斯方法常用条件独立假设：$$P(X_1, X_2, \dots, X_n | C) = \prod_{i=1}^n P(X_i | C)$$
- 简化计算，避免高维联合概率计算
- 朴素贝叶斯的核心假设，即使在特征不完全独立时，模型也常表现良好。

## 极大似然估计与贝叶斯推断
在机器学习中，模型参数通常是未知的，需要通过观测数据来进行估计。极大似然估计（MLE）和贝叶斯推断是两种最核心的参数估计方法，它们分别从不同角度回答**如何根据数据选择或更新参数**的问题。
### 极大似然估计（Maximum Likelihood Estimation, MLE）
**基本思想**
MLE 的核心思想是：选择一组参数，使观测数据出现的可能性最大。
- 设数据集为 $$D = \{x_1, x_2, \dots, x_n\}$$
- 参数为 $$\theta$$
- 概率模型为 $$P(x|\theta)$$
极大似然估计公式为：$$\hat{\theta}_{MLE} = \arg \max_\theta \prod_{i=1}^n P(x_i|\theta)$$

通常为了方便计算，取对数得到对数似然函数：$$\hat{\theta}_{MLE} = \arg \max_\theta \sum_{i=1}^n \log P(x_i|\theta)$$

对数似然函数将乘积转为求和，数值稳定且便于求导。
**示例**
伯努利分布（如逻辑回归中的二分类）：
- 假设样本 $$x_i \in \{0,1\}$$，概率 $$P(x_i=1|\theta)=\theta$$
- 对数似然函数：$$\ell(\theta) = \sum_{i=1}^n \Big[ x_i \log \theta + (1-x_i)\log(1-\theta) \Big]$$
- 求导并令其为 0，可得到 MLE 解：$$\hat{\theta}_{MLE} = \frac{\sum_{i=1}^n x_i}{n}$$

直观理解就是MLE 选择使观测数据中“正例比例”最大化的参数。

### 贝叶斯推断（Bayesian Inference）
**基本思想**

贝叶斯推断强调结合先验知识和观测数据来更新对参数的信念。
- 设先验分布为 $$P(\theta)$$
- 观测数据的似然为 $$P(D|\theta)$$
- 后验分布为：$$P(\theta|D) = \frac{P(D|\theta) P(\theta)}{P(D)}$$
其中$$P(D) = \int P(D|\theta) P(\theta) d\theta$$是归一化常数。

**与 MLE 的关系**
- 当先验 $$P(\theta)$$ 是均匀分布时，贝叶斯推断的最大后验估计（MAP）与 MLE 等价：
$$\hat{\theta}_{MAP} = \arg \max_\theta P(\theta|D) = \arg \max_\theta P(D|\theta)$$
- 与 MLE 不同，贝叶斯推断给出的是整个后验分布，不仅仅是一个点估计。

**示例**

贝塔先验 + 伯努利观测：
- 先验： $$\theta \sim \text{Beta}(\alpha, \beta)$$ 
- 似然：伯努利分布 $$P(D|\theta) = \theta^{\sum x_i} (1-\theta)^{n-\sum x_i}$$
- 后验：$$\theta|D \sim \text{Beta}\Big(\alpha + \sum x_i, \beta + n - \sum x_i \Big)$$

后验分布直接结合了先验知识和观测数据，MAP 估计为：$$\hat{\theta}_{MAP} = \frac{\alpha + \sum x_i - 1}{\alpha + \beta + n - 2}$$

### 在机器学习中的应用
**线性回归**: MLE 对残差服从高斯分布时，等价于最小二乘法

**逻辑回归**: MLE 用于估计权重参数

**贝叶斯分类器**: 通过后验分布进行预测

**生成模型与概率图模型**: MLE 用于参数拟合，贝叶斯方法用于推断隐藏变量和不确定性

**贝叶斯优化**: 利用后验分布更新目标函数估计，实现高效超参数搜索


最新的文章都在公众号更新，别忘记关注哦！！！如果想要加入技术群聊，扫描下方二维码回复【加群】即可。
![](./imgs/coting_qrcode.png)